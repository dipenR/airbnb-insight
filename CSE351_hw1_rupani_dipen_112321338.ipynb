{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "e28730d61f4f668eae55e36873d8951e5b131a7fffb868017a1cee4d51b73dfb"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# IMPORTS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import geopandas as gpd \n",
    "import contextily as ctx\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import nltk, gensim\n",
    "nltk.download('punkt')\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, ImageColorGenerator"
   ]
  },
  {
   "source": [
    "## READ DATA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('AB_NYC_2019.csv')"
   ]
  },
  {
   "source": [
    "# PART 1- READ, EXAMINE, AND CLEAN DATA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## DATA EXPLORATION"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes, data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "source": [
    "## FILLING MISSING VALUES"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['reviews_per_month'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check if all reviews_per_month null values actually just have no reviews\n",
    "data[(data['reviews_per_month'].isna()) & (data['number_of_reviews'] != 0)]"
   ]
  },
  {
   "source": [
    "### Now we know that for all missing values in **reviews_per_month**, they should just be 0"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reviews_per_month.fillna(0, inplace=True)"
   ]
  },
  {
   "source": [
    "### Now let us do **last_review**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[(data['last_review'].isna()) & (data['number_of_reviews'] != 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also the same - all last_review NaN have never received a review, ever. However, there is no useful way to fill this data out and it will be of very little use for oure future analysis,=. So we will drop this column\n",
    "data.drop('last_review', axis=1, inplace=True)"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Now lets deal with **host_name** and **name**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.host_name.isnull().sum(), data.name.isnull().sum()"
   ]
  },
  {
   "source": [
    "We have no way to know what their real names or host anmes are, and the number of these records are few so we will simply delete these records since filling htem in any other way might affect our word clouds in part 5"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True) # note that since we already filled out the values that we wanted to, this will only delete the union(16, 21) missing values."
   ]
  },
  {
   "source": [
    "### Expecation: No more null values."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "source": [
    "## OUTLIER DETECTION"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Already, we can see some anomlies with the data - min price = 0 max price = 10000, min number of nights = 1250, min availability_365 = 0."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### price"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data.price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us define outliers as anything less than the 1st percentile and anything more than the 99th percentile. Then our range becomes\n",
    "data.price.quantile(0.01), data.price.quantile(0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The upper limit comes out to be 355, however, in part 4 we need to see all prices upto 1000 so let us use that as the upper limit\n",
    "sns.distplot(data[data['price'] < 800]['price'])"
   ]
  },
  {
   "source": [
    "This looks much better. Note that the price density peaks at close to 50-80, and each peak after than can be explained by the fact that people like to enter whole numbers as prices. A price of 150 or 200 sounds better than 156.62."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = data[(data['price']>=30) & (data['price'] <= 800)]"
   ]
  },
  {
   "source": [
    "#### minimum_nights"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data.minimum_nights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us do the same thing again since there are many listings with very high minimum number of nights\n",
    "data.minimum_nights.quantile(0.01), data.minimum_nights.quantile(0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = clean_data[(clean_data['minimum_nights']>0) & (clean_data['minimum_nights']<=45)]"
   ]
  },
  {
   "source": [
    "note that we calculate these quantiles based on the larger dataset because after cleaning each feature we have removed some data records hence affecting the means of other data. We do not want to remove any more data than necessary."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(clean_data.minimum_nights)"
   ]
  },
  {
   "source": [
    "This also looks somewhat better than the previous plot. The peak at 30 days can be explined by people who just want to give their home out on amonthly rent basis."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### availability_365"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(clean_data.availability_365)"
   ]
  },
  {
   "source": [
    "Why so many people with 0 availability? -- Maybe becaus they are inactive/suspended/banned accounts. I believe that there are far too many records with 0 availability that, unfortuately, we will not be able to remove them. Se let us just let them remain."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## FEATURE ENGINEERING\n",
    "### Some extra features created as needed for practical purposes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. log_price - we introduce this to see its affect on corelation between different variables\n",
    "clean_data['log_price'] = np.log10(np.log10(clean_data['price']))"
   ]
  },
  {
   "source": [
    "--------------"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# PART 2 - EXAMINE PRICE CHANGES"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Let us start by first finding top 5 and bottom 5 neighbourhoods by price, using only neighbourhoods that have more than 5 listings."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by selecting only those neighbourhoods that have occured more than 5 times, first get all counts and then select only those rows which have neighbourhoods that have occured more than 5 times.\n",
    "neighbourhood_counts = clean_data.neighbourhood.value_counts()\n",
    "neighbourhood_data = clean_data[clean_data.neighbourhood.isin(neighbourhood_counts.index[neighbourhood_counts.gt(5)])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine our new data\n",
    "neighbourhood_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbourhood_data.groupby(['neighbourhood_group'], as_index=False).agg({'price':['mean']}).sort_values(by=('price', 'mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbourhood_group_data = neighbourhood_data.groupby(['neighbourhood_group', 'neighbourhood'], as_index=False).agg({'price':['mean']})"
   ]
  },
  {
   "source": [
    "### Now clearly, top 5 and bottom 5 neighbourhoods are: \n",
    "### Bottom: Hunts Point, Tremont, Soundview, Bronxdale, Concord\n",
    "### Top: Tribeca, NoHp, Flatiron District, Midtown, West Village\n",
    "### And the boroughs are in the order: Bronx > Staten Island > Queens  > Brooklyn > Manhattan\n",
    "### Let us plot some of these to see trends\n",
    "### Let us divide the data by neighbourhood groups"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotted to prove a point - this graph is not a good graph\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.xticks(rotation=90)\n",
    "plt.bar(neighbourhood_group_data['neighbourhood'], neighbourhood_group_data[('price', 'mean')])\n",
    "plt.xlabel('borough')\n",
    "plt.ylabel('mean price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clearly the above graph is mess, we will instead group by borough and plot prices for each neighbourhood\n",
    "bronx_data = neighbourhood_group_data[neighbourhood_group_data['neighbourhood_group'] == 'Bronx']\n",
    "queens_data = neighbourhood_group_data[neighbourhood_group_data['neighbourhood_group'] == 'Queens']\n",
    "statenI_data = neighbourhood_group_data[neighbourhood_group_data['neighbourhood_group'] == 'Staten Island']\n",
    "brooklyn_data = neighbourhood_group_data[neighbourhood_group_data['neighbourhood_group'] == 'Brooklyn']\n",
    "manhattan_data = neighbourhood_group_data[neighbourhood_group_data['neighbourhood_group'] == 'Manhattan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,5))\n",
    "plt.bar(neighbourhood_group_data['neighbourhood_group'], neighbourhood_group_data[('price', 'mean')])\n",
    "plt.xlabel('borough')\n",
    "plt.ylabel('mean price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,5))\n",
    "plt.xticks(rotation=90)\n",
    "plt.bar(bronx_data['neighbourhood'], bronx_data[('price', 'mean')])\n",
    "plt.xlabel('borough')\n",
    "plt.ylabel('mean price')"
   ]
  },
  {
   "source": [
    "Observations: Highest mean price: Riverdale, lowest mean price: Hunt's point"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This looks much better, let us now do the same for other boroughs: \n",
    "plt.figure(figsize=(16,5))\n",
    "plt.xticks(rotation=90)\n",
    "plt.bar(statenI_data['neighbourhood'], statenI_data[('price', 'mean')])\n",
    "plt.xlabel('borough')\n",
    "plt.ylabel('mean price')"
   ]
  },
  {
   "source": [
    "Observations: Very few neighbourhoods, highest mean price: Grymes Hill, lowest mean price: Concord"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,5))\n",
    "plt.xticks(rotation=90)\n",
    "plt.bar(queens_data['neighbourhood'], queens_data[('price', 'mean')])\n",
    "plt.xlabel('borough')\n",
    "plt.ylabel('mean price')"
   ]
  },
  {
   "source": [
    "Observations: Very high density of neighbourhoods, highest mean price: Belle Harbor, lowest mean price: Corona"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,5))\n",
    "plt.xticks(rotation=90)\n",
    "plt.bar(brooklyn_data['neighbourhood'], brooklyn_data[('price', 'mean')])\n",
    "plt.xlabel('borough')\n",
    "plt.ylabel('mean price')"
   ]
  },
  {
   "source": [
    "Observations: highest mean price: DUMBO, lowest mean price: Borough park"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,5))\n",
    "plt.xticks(rotation=90)\n",
    "plt.bar(manhattan_data['neighbourhood'], manhattan_data[('price', 'mean')])\n",
    "plt.xlabel('borough')\n",
    "plt.ylabel('mean price')"
   ]
  },
  {
   "source": [
    "Observations: highest mean price: Tribeca, lower mean price: Inwood"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "-----------------------"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# PART 3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Features selected: log_price, longitude, minimum_nights, number_of_reviews, reviews_per_month, calculated_host_listings_count, and availability_365. These features have been selected asa they are quite diverse and any correlation between any 2 variables will be quite interesting."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hm = sns.heatmap(clean_data[['log_price', 'longitude', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365']].corr(), annot=True)\n",
    "hm.set(xlabel = 'features', ylabel = 'features')"
   ]
  },
  {
   "source": [
    "Aside from the obvious correlation between number of reviews and reviews per month, which is unintersting, we have the following interesting correlation:\n",
    "- log_price and longitude\n",
    "\n",
    "- minimum_nights and calculated_host_listings_count - people that buy houses particularly for airbnb and other renting, dont live in that house hence increaing minimum_nights and buy a lot of properties - hence increasing their calculated_host_listings_count\n",
    "\n",
    "- reviews_per_month and availability_365 - They could be given out mroe often hence increasing their reviews\n",
    "\n",
    "- availability_365 and minimum_nights - This might be explained by the fact the people that dont live in that plot probably want people to stay for long - sort of like a rent system\n",
    "\n",
    "We should keep in mind that for such a  large dataset, even small correlations (>=|0.2|) is also significant."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# PART 4"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us make a geopandas dataframe to make use of latitude and longitude\n",
    "geodata = gpd.GeoDataFrame(clean_data[['latitude', 'longitude', 'neighbourhood_group']], geometry = gpd.points_from_xy(clean_data.longitude, clean_data.latitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the data\n",
    "geodata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a built in map ot nyc\n",
    "nyc = gpd.read_file(gpd.datasets.get_path('nybb')) #geopandas file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the shape of nyc for reference for our later plot\n",
    "nyc.plot(figsize=(10,10),color='gray', alpha=0.5, edgecolor='black') # plot to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cool plot :) -- not necessary  PLEASE NOTE - THIS CELL MAY NEED TO BE RUN TWICE DUE TOA BUG IN GEOPANDAS PLOTTING\n",
    "nyc_plot = nyc.plot(figsize=(10,10),color='gray', alpha=0.5, edgecolor='black') # plot to work with\n",
    "nyc = nyc.to_crs(epsg=3857)\n",
    "ctx.add_basemap(nyc_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geodata.plot(figsize=(15, 15), color='red', markersize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a label encoder to go from bonrx -> 0, Staten Island -> 1, etc\n",
    "encoder = LabelEncoder()\n",
    "geodata['neighbourhood_group'] = encoder.fit_transform(geodata['neighbourhood_group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "colors = geodata['neighbourhood_group'].unique()\n",
    "rgb_values = sns.color_palette('Set1', 8)\n",
    "color_map = dict(zip(colors, rgb_values))\n",
    "plt.figure(figsize=(15,15), dpi=150)\n",
    "# plt.figure()\n",
    "plt.scatter(geodata['longitude'], geodata['latitude'], c=geodata['neighbourhood_group'].map(color_map))"
   ]
  },
  {
   "source": [
    "# PART 5"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate features from name\n",
    "name_list = list(clean_data['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "for name in name_list:\n",
    "    tokens = nltk.word_tokenize(name) # tokenize each record\n",
    "    cleaned_tokens = [token for token in tokens if token.isalnum()] # only take al-num, not !!, .., etc\n",
    "    text += \" \".join(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(max_font_size=60, max_words=200, background_color='white').generate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.figure(figsize=(15,15), dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "# PART 6"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us start by examining host_id\n",
    "len(clean_data['host_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_data = clean_data.groupby(['host_id']).agg({'host_id':['count'], 'price': ['mean', 'max', 'min'], 'availability_365': ['mean'], 'number_of_reviews': ['mean']}).sort_values(by=('host_id','count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_data # note that host_id is the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us see some of these hosts:\n",
    "clean_data[clean_data['host_id'] == 219517861]"
   ]
  },
  {
   "source": [
    "According to the data, this host in the financial district has the most number of listings. Observations: It looks like these listings are more houses up for rent, considering that they are all up for 29 days minimum. \n",
    "\n",
    "It also looks like these houses are up for 327/365 days suggesting that the other days may be spent either living in them or maintaining these houses at different times of the year."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm = sns.heatmap(host_data.corr(), annot=True)\n",
    "hm.set(xlabel = 'features', ylabel = 'features')"
   ]
  },
  {
   "source": [
    "The above heatmap clearly shows a positive correlation between availability_365 and number_of_reviews - the more often the listing is avialable -> the more often people stay in it -> the more often they review it. \n",
    "\n",
    "Hence the busiest hosts are those that have most plots available for the most ammount of time\n",
    "\n",
    "Note: the prices are correlated for obvious reasons - they are derived from the same variable"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# PART 7"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "For our first plot, let us observe the availability in the different boroughs dependin on room type"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,10))\n",
    "sns.boxplot(x='neighbourhood_group', y='availability_365', hue='room_type', data=clean_data)"
   ]
  },
  {
   "source": [
    "This plot is clearly interesting because it shows-\n",
    "- shared rooms are available far more frequently than any other type of room. This makes sense because it does not require the host to leave or have a separate home.\n",
    "- Staten ISland has a disproportionately small number of shared rooms while having more availability in their private rooms as well as home/apt. This could have a socio-economic backing with people only have secondary houses in Staten Island and prefering to lend them out as it is closer to the beach."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "For our second plot let us observe the difference removing the most common words will make on our word cloud."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate features from name\n",
    "name_list = list(clean_data['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "for name in name_list:\n",
    "    tokens = nltk.word_tokenize(name) # tokenize each record\n",
    "    cleaned_tokens = [token for token in tokens if token.isalnum()] # only take al-num, not !!, .., etc\n",
    "    text.append(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dictionary = gensim.corpora.Dictionary(text)\n",
    "corpus_dictionary.filter_extremes(no_below=10, no_above=0.5) # filter out words that have occured less than 5 times and filter out top 50% of the most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dict = dict(corpus_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = \"\"\n",
    "for k,v in corpus_dict.items():\n",
    "     all_text += \" \" + v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(max_font_size=60, max_words=100, background_color='white').generate(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.figure(figsize=(15,15), dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "The word cloud seems to have become more about the stay thant he place of stay. In the previous word cloud we could see things like Brooklyn, Manhattan, etc. Now we don't. This tells us that most of the host_name lisitngs had the place of stay in them and since we removed to top 50% of the vocabulary, we don't see those words anymore."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}